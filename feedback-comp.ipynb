{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Notes\n## Papers\n* [Text Segmentation as a Supervised Learning Task](https://arxiv.org/pdf/1803.09337.pdf)","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-04T04:09:34.519468Z","iopub.execute_input":"2022-01-04T04:09:34.520467Z","iopub.status.idle":"2022-01-04T04:09:37.558735Z","shell.execute_reply.started":"2022-01-04T04:09:34.520340Z","shell.execute_reply":"2022-01-04T04:09:37.558034Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"# Importing and exploring data","metadata":{}},{"cell_type":"code","source":"pd_annotations = pd.read_csv('/kaggle/input/feedback-prize-2021/train.csv')\ndocument_ids = list(pd_annotations['id'].unique())\n\nnum_documents = 100\n\npd_annotations.head()","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:09:37.560571Z","iopub.execute_input":"2022-01-04T04:09:37.560831Z","iopub.status.idle":"2022-01-04T04:09:39.593405Z","shell.execute_reply.started":"2022-01-04T04:09:37.560799Z","shell.execute_reply":"2022-01-04T04:09:39.592715Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"             id  discourse_id  discourse_start  discourse_end  \\\n0  423A1CA112E2  1.622628e+12              8.0          229.0   \n1  423A1CA112E2  1.622628e+12            230.0          312.0   \n2  423A1CA112E2  1.622628e+12            313.0          401.0   \n3  423A1CA112E2  1.622628e+12            402.0          758.0   \n4  423A1CA112E2  1.622628e+12            759.0          886.0   \n\n                                      discourse_text discourse_type  \\\n0  Modern humans today are always on their phone....           Lead   \n1  They are some really bad consequences when stu...       Position   \n2  Some certain areas in the United States ban ph...       Evidence   \n3  When people have phones, they know about certa...       Evidence   \n4  Driving is one of the way how to get around. P...          Claim   \n\n  discourse_type_num                                   predictionstring  \n0             Lead 1  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...  \n1         Position 1       45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \n2         Evidence 1    60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75  \n3         Evidence 2  76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...  \n4            Claim 1  139 140 141 142 143 144 145 146 147 148 149 15...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>discourse_id</th>\n      <th>discourse_start</th>\n      <th>discourse_end</th>\n      <th>discourse_text</th>\n      <th>discourse_type</th>\n      <th>discourse_type_num</th>\n      <th>predictionstring</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>8.0</td>\n      <td>229.0</td>\n      <td>Modern humans today are always on their phone....</td>\n      <td>Lead</td>\n      <td>Lead 1</td>\n      <td>1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 1...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>230.0</td>\n      <td>312.0</td>\n      <td>They are some really bad consequences when stu...</td>\n      <td>Position</td>\n      <td>Position 1</td>\n      <td>45 46 47 48 49 50 51 52 53 54 55 56 57 58 59</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>313.0</td>\n      <td>401.0</td>\n      <td>Some certain areas in the United States ban ph...</td>\n      <td>Evidence</td>\n      <td>Evidence 1</td>\n      <td>60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>402.0</td>\n      <td>758.0</td>\n      <td>When people have phones, they know about certa...</td>\n      <td>Evidence</td>\n      <td>Evidence 2</td>\n      <td>76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 9...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>423A1CA112E2</td>\n      <td>1.622628e+12</td>\n      <td>759.0</td>\n      <td>886.0</td>\n      <td>Driving is one of the way how to get around. P...</td>\n      <td>Claim</td>\n      <td>Claim 1</td>\n      <td>139 140 141 142 143 144 145 146 147 148 149 15...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Let's look at txt with id '0000D23A521A'\nprint(pd_annotations[pd_annotations.id == '0000D23A521A']['predictionstring'].iloc[0])\nprint(pd_annotations[pd_annotations.id == '0000D23A521A']['predictionstring'])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:09:39.594764Z","iopub.execute_input":"2022-01-04T04:09:39.595003Z","iopub.status.idle":"2022-01-04T04:09:39.647178Z","shell.execute_reply.started":"2022-01-04T04:09:39.594971Z","shell.execute_reply":"2022-01-04T04:09:39.646115Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33\n59951    0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...\n59952    34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 4...\n59953         69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n59954    84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 9...\n59955    117 118 119 120 121 122 123 124 125 126 127 12...\n59956    134 135 136 137 138 139 140 141 142 143 144 14...\n59957    154 155 156 157 158 159 160 161 162 163 164 16...\n59958    186 187 188 189 190 191 192 193 194 195 196 19...\nName: predictionstring, dtype: object\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Segmentation\nLet's start with segmenting the text. We'll work on classifying those segments later.\n\nPer Koshorek et al:\n> Input \\\\(x\\\\) will be represented as a sequence of \\\\(n\\\\) sentences \\\\(s_1,...,s_n\\\\), and the label \\\\(y = (y_1,...,y_{n-1})\\\\) is a segmentation of the document, represented by \\\\(n-1\\\\) binary values, where \\\\(y_i\\\\) denotes whether \\\\(s_i\\\\) ends a segment.\n\n\\\\(y_i = 0\\\\) indicates \\\\(s_i\\\\) is not a border, while a value of \\\\(y_i = 1\\\\) indicates \\\\(s_i\\\\) is a border sentence.","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"\nWHITE_SPACE_CHARS = [' ', '\\t', '\\n', '\\r', '\\x0b', '\\x0c']\nfrom transformers import BertTokenizerFast\n\n# This will let us grab text and an associated label. 'O' means \"outside\" of any relevant segments.\ndef get_entities(text, doc_id):\n    token_array = np.zeros(298)\n    char_array = np.zeros((len(text)))\n    \n    entities = []\n    \n    pd = pd_annotations[pd_annotations['id'] == doc_id]\n    \n    start_i = [int(row) for row in list(pd['discourse_start'])]\n    end_i = [int(row) for row in list(pd['discourse_end'])]\n    annotations = [row for row in list(pd['discourse_type'])]\n    \n    print(start_i)\n    text_i = 0\n    for i in range(len(start_i)):\n        if text_i < start_i[i]:\n            entities.append((text[ text_i : start_i[i] ], 'O'))\n            text_i = start_i[i]\n        entities.append((text[ start_i[i] : end_i[i] ], annotations[i]))\n        text_i = end_i[i]\n    return entities\n\nall_entities = {}\nfor uid in document_ids[0:num_documents]:\n    doc = open(f'/kaggle/input/feedback-prize-2021/train/{uid}.txt').read()\n    all_entities[uid] = get_entities(doc, uid)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:09:39.648626Z","iopub.execute_input":"2022-01-04T04:09:39.648891Z","iopub.status.idle":"2022-01-04T04:09:48.667668Z","shell.execute_reply.started":"2022-01-04T04:09:39.648856Z","shell.execute_reply":"2022-01-04T04:09:48.666937Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"[8, 230, 313, 402, 759, 887, 1151, 1534, 1603, 1891]\n[18, 86, 203, 1031]\n[36, 513, 594, 727, 1245, 1440, 1682]\n[0, 455, 528, 569, 589, 782, 937, 1404, 1507, 2043, 2188, 2878]\n[21, 380, 461, 541, 955, 1052, 1530, 1604, 1914]\n[52, 200, 245, 295, 418, 516, 590, 1162, 1220, 1718, 1843, 2444]\n[24, 95, 295, 546, 682, 1009, 1359, 1480, 1945]\n[0, 429, 544, 725, 1333, 1498, 1870, 2535]\n[63, 129, 252, 1030, 1161, 1350, 1512, 1859]\n[0, 779, 1000, 1118, 1351, 1467]\n[0, 223, 295, 319, 693, 785, 937]\n[0, 78, 634, 976, 1525, 1637, 2287]\n[0, 462, 532, 641, 838, 999, 1348, 1455, 1641]\n[0, 68, 108, 150, 199, 274, 1048]\n[20, 371, 477, 1322, 1436, 1498, 2131, 2256, 3162, 3218]\n[0, 360, 554, 688, 767, 777, 802, 943, 1285, 1390, 1910, 2005, 2510, 2675]\n[0, 608, 719, 879, 1284, 1368]\n[0, 145, 393, 686, 861, 953, 1425, 1487, 1889]\n[32, 544, 637, 696, 726, 763, 872, 1437, 1601, 2161, 2335, 2842]\n[20, 418, 484, 997, 1211, 1384, 1724, 1786]\n[0, 201, 309, 942, 1299, 1450, 2027, 2155, 2711]\n[17, 260, 317, 357, 399, 428, 537, 596, 845, 1002, 1224, 1317, 1652]\n[36, 251, 407, 438, 453, 479, 544, 1588, 1760, 2057]\n[18, 584, 678, 730, 1138, 1283, 1876, 1924, 2455]\n[51, 252, 307, 382, 410, 425, 532, 1061, 1164, 1672, 1722, 2511]\n[0, 310, 445, 583, 923, 1210, 1330, 1469]\n[0, 584, 678, 871, 1338, 1431, 2004, 2084, 2142, 2646]\n[0, 73, 162, 271, 334, 359, 400, 528, 1023, 1222, 1671, 1787, 2338]\n[27, 115, 157, 609, 687, 1352]\n[0, 463, 586, 749, 904, 1628]\n[28, 80, 139, 234, 487, 779, 955]\n[18, 216, 344, 507, 798, 866, 1485]\n[0, 75, 415, 620, 822, 1168, 1365, 1572, 1960]\n[18, 144, 177, 227, 303, 447, 512, 562, 606, 724, 1105, 1184, 1378, 1461]\n[20, 116, 215, 369, 545, 719, 854, 1004]\n[44, 444, 501, 612, 1301, 1379, 2377, 2630, 2756]\n[20, 125, 459, 563, 746, 792, 1112, 1256, 1482]\n[21, 240, 285, 472, 608, 696, 1257, 1436, 2121, 2414, 2951]\n[18, 97, 152, 346, 560, 689, 1202, 1560]\n[0, 74, 183, 313, 619, 757, 1188, 1405, 1618, 1762, 2090]\n[0, 895, 1034, 1281, 1645, 2232, 2366]\n[20, 562, 665, 779, 944, 1085, 1133, 1206, 1301, 1372, 1574, 1686, 2144, 2444]\n[0, 154, 218, 264, 293, 411, 661, 731, 848, 1004, 1203, 1305, 1335]\n[0, 89, 212, 377, 579, 711, 1039, 1134, 1383]\n[0, 271, 460, 638, 927, 1095, 1591, 1679, 1839, 1900]\n[18, 130, 1003, 1184, 2566, 2718, 3172]\n[20, 105, 272, 372, 1131, 1319, 2327, 2500, 3561]\n[0, 197, 365, 504, 853, 1197, 1314]\n[20, 457, 529, 1179, 1464, 1574, 1819, 1911]\n[17, 89, 157, 197, 216, 277, 498, 639, 986, 1079, 1538, 1586, 2002]\n[21, 173, 259, 496, 621, 827, 989, 1037, 1279, 1364, 1840, 1995, 2082, 2553]\n[20, 66, 460, 519, 1035, 1273, 1428, 1508, 1842]\n[0, 144, 212, 254, 301, 351, 626, 840, 992, 1094, 2012, 2149, 2657, 2824, 2942, 3310]\n[0, 130, 581, 624, 1021, 1125, 1398]\n[34, 473, 606, 688, 825, 1126, 1216, 1408]\n[21, 193, 282, 342, 377, 499, 1630, 1762, 2527]\n[20, 262, 370, 463, 586, 746, 844, 973]\n[18, 413, 510, 648, 848, 1040]\n[20, 182, 249, 286, 596, 632, 802, 846, 1113]\n[0, 236, 290, 334, 388, 425, 521, 627, 934, 975, 1353, 1405, 1777, 1920]\n[20, 134, 333, 659, 810, 1336]\n[20, 115, 148, 183, 219, 284, 363, 875, 932, 1333, 1392, 1746]\n[0, 45, 205, 639, 800, 1120, 1216, 1351, 1589, 1710, 1875, 2096]\n[0, 104, 216, 377, 452, 718, 861, 1117, 1257, 1456]\n[25, 89, 176, 250, 302, 352, 985, 1078, 1623, 1727, 2193]\n[0, 100, 250, 951, 1044, 1628]\n[20, 333, 547, 630, 1122, 1184, 2079, 2149, 2823, 3480, 3631]\n[18, 566, 620, 706, 1264, 1417, 1840, 2456, 2564, 2696, 2815]\n[0, 85, 145, 267, 432, 528, 971, 1086, 1455, 1496, 1824]\n[0, 656, 725, 808, 1142, 1202, 1430, 1507, 1871]\n[44, 128, 315, 606, 794, 919, 1595]\n[17, 133, 229, 324, 513, 613, 1156, 1201]\n[23, 846, 971, 1361, 1408, 1865, 1898, 2209, 2366, 2601]\n[20, 378, 430, 513, 729, 798, 1134, 1216, 1417, 1711]\n[35, 605, 697, 1390, 1855, 2008, 2293, 2496, 2618, 3064, 3463]\n[26, 278, 353, 458, 898, 993, 1510]\n[1, 283, 383, 528, 784, 931, 985, 1384, 1506, 2054]\n[19, 343, 486, 583, 966, 1069, 1391, 1477, 1891]\n[0, 122, 228, 300, 331, 385, 521, 853, 915, 1380, 1496, 1586]\n[33, 400, 491, 719, 1187, 1307, 1503, 2077, 2204, 3178]\n[0, 519, 569, 669, 976, 1363]\n[20, 493, 575, 705, 1194, 1306, 2225]\n[20, 489, 605, 720, 1484]\n[0, 787, 980, 1115, 1753, 1910, 2489, 2709, 3293]\n[24, 90, 132, 166, 248, 272, 399, 532, 808, 998, 1142, 1599, 1871, 2052, 2246]\n[0, 332, 430, 847, 921, 1036, 1085, 1230, 1462, 1555, 1615, 1865]\n[17, 694, 846, 1013, 1131, 1501, 1587, 1727, 2359, 2469, 2614, 3140]\n[0, 242, 334, 1105, 1147, 1300, 1815, 1880, 2320]\n[0, 213, 289, 314, 335, 369, 433, 492, 1032, 1111]\n[71, 684, 795, 1782, 1856, 2746]\n[23, 130, 204, 666, 976]\n[48, 564, 707, 882, 1911, 2020, 2931]\n[20, 251, 327, 366, 391, 467, 1181, 1297, 2308]\n[17, 818, 954, 1142, 1583, 1710, 2390, 2545, 3110]\n[19, 424, 482, 513, 595, 715, 1219, 1332, 1666, 1813, 1952]\n[14, 232, 566, 719, 945, 1148, 1481, 1850, 2464]\n[0, 45, 128, 275, 452, 811, 880, 923, 1320, 1409, 1590]\n[0, 657, 866, 1058, 1463, 1588, 1835, 2124, 2224, 2870]\n[0, 214, 303, 364, 821, 884, 1282]\n[20, 118, 564, 728, 886, 1119, 1175, 1616, 1806, 1971, 2328]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Text preprocessing\nNow we have a bunch of segments. Let's go ahead and get them ready for encoding.\n* Lematize\n* Tokenize\n* word2vec embeddings","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\nlemmatizer = WordNetLemmatizer()\n\npreprocessed_entities = {}\n\nfor uid in all_entities:\n    preprocessed_entities[uid] = []\n    for entity in all_entities[uid]:\n        tokenized = word_tokenize(entity[0])\n        tokenized = [word for word in tokenized if word not in stop_words]\n        lemmatized = [lemmatizer.lemmatize(token) for token in tokenized]\n        preprocessed_entities[uid].append(lemmatized)\n\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:09:48.670181Z","iopub.execute_input":"2022-01-04T04:09:48.670807Z","iopub.status.idle":"2022-01-04T04:09:52.531037Z","shell.execute_reply.started":"2022-01-04T04:09:48.670768Z","shell.execute_reply":"2022-01-04T04:09:52.530330Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# After all texts preprocessed, we want to build up our dictionary for one hot encoding\n\nindex_to_word = []\nword_to_index = {}\n\nfor uid in preprocessed_entities:\n    for entity in preprocessed_entities[uid]:\n        for token in entity:\n            if token not in index_to_word:\n                index_to_word.append(token)\n                \nfor i in range(len(index_to_word)):\n    word_to_index[index_to_word[i]] = i\n\n# How many words are there in the corpus?\nnum_input_tokens = len(index_to_word)\nnum_documents = len(preprocessed_entities)\nmax_token_length = max([sum([len(entity) for entity in preprocessed_entities[uid]]) for uid in preprocessed_entities])\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:09:52.532294Z","iopub.execute_input":"2022-01-04T04:09:52.532549Z","iopub.status.idle":"2022-01-04T04:09:52.692817Z","shell.execute_reply.started":"2022-01-04T04:09:52.532508Z","shell.execute_reply":"2022-01-04T04:09:52.692094Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"DISCOURSE_TYPES = {'O': 0, 'Lead': 2, 'Position': 4, 'Claim': 6, 'Counterclaim': 8, 'Rebuttal': 10, 'Evidence': 12, 'Concluding Statement': 14}\nIBO_LABELS = {'O': 0, 'LB': 1, 'LI': 2, 'PB': 3, 'PI': 4, 'CB': 5, 'CI': 6, 'XB': 7, 'XI': 8, 'RB': 9, 'RI': 10, 'EB':11, 'EI': 12, 'SB': 13, 'SI': 14}\n\n# Build the target array\ntargets = {}\nfor uid in preprocessed_entities:\n    targets[uid] = []\n    for i in range(len(preprocessed_entities[uid])): # For each entity\n        for j in range(len(preprocessed_entities[uid][i])): # For each token\n            annotation = all_entities[uid][i][1]\n            label = DISCOURSE_TYPES[annotation]\n            \n            if j == 0 and label > 0:\n                label -= 1\n                \n            targets[uid].append(label)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:10:48.912842Z","iopub.execute_input":"2022-01-04T04:10:48.913147Z","iopub.status.idle":"2022-01-04T04:10:48.941666Z","shell.execute_reply.started":"2022-01-04T04:10:48.913115Z","shell.execute_reply":"2022-01-04T04:10:48.940882Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# One hot encoding for input and output\n\n\n\none_hot_input = np.zeros((num_documents, max_token_length, num_input_tokens))\ny = np.zeros((num_documents, max_token_length, len(IBO_LABELS)))\n\nfor i, uid in enumerate(preprocessed_entities):\n    k = 0\n    print(uid)\n    for entity in preprocessed_entities[uid]:\n        for token in entity:\n            one_hot_input[i][k][word_to_index[token]] = 1\n            y[i][k][targets[uid][k]] = 1\n            k += 1","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:10:48.943396Z","iopub.execute_input":"2022-01-04T04:10:48.943735Z","iopub.status.idle":"2022-01-04T04:10:49.088879Z","shell.execute_reply.started":"2022-01-04T04:10:48.943696Z","shell.execute_reply":"2022-01-04T04:10:49.088103Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"423A1CA112E2\nA8445CABFECE\n6B4F7A0165B9\nE05C7F5C1156\n50B3435E475B\nDBF7EB6A9E02\n810B70E80E1D\nCE98789F502B\nA97DE0D49AEA\n48D3F4243F0F\nAC594194F01C\n4F0E197053FF\nC3811E7F1750\n86C1ED49C35F\n019328A0D7A3\n4B6C254FEE39\nF054050F442F\n4C30EEDA3A8F\n20FD63F49519\n21730F71662E\nA783D3241786\nDE628D1F2F9D\n1B1FA8C3F4F9\n2E98ECF2CA61\nD0CBBD43827C\nE34D7384EE70\nCC296299ABA4\n62644C50869C\nD7D83D1EBFDB\n7E29804EE125\n1CCC2C060AA2\n0421128DEE6C\n42048FB6EC2B\n7FF6281EC288\n5B8AD3907163\n89808E74DDC5\n77FADB16D0F4\n2F2607C7D8F7\nB7F586D0260C\n326B272D36A0\n4B30291A725D\n4662057D0A77\nE527586F851C\n5F1CF4B91975\nF425F44374DD\n5613F9FB2154\nD14A82EE41BF\n8BFC5B17C5AF\n077DD935321C\nAA994A6CAF65\n6A2F708CAA8B\nD59E1F10092B\n354946A1CA46\nF3E71A1A4F8B\nDA1C845AB04A\n743904BAD7E5\n3828201E7783\nC30B52D6E340\n57E2E8E20B45\n1D35A6980E7F\n41EF348E3016\n9F5A37599E7B\n40CC76613B2D\n4000B8222A07\n2022539CFE3E\n15128715053E\nE3830AB95CD7\nE92185894096\n9B45D0A9E4C0\n3DCD0C034E88\nC7A316555DF7\nC161EEA83234\n6B2C2AFDFC90\n2C42788D171D\nD5D31918A943\nB0E93CC3E195\n66BD5DA864C8\n5FD1A8FB7F6C\n29F3D2561639\nCB86EB780CF1\n9FCC32B37608\n46425944AD00\n4B361B943DFD\nDE01C3FDCA0A\n9DD7A21F7329\n570D8769BE33\n63E8EF40EF1D\nAC71A062B952\n97CF45004451\nE3ED7398948E\n2DFFBEFF2B10\nE5E3385C072A\n2E2C32380220\n6926AE8EDFD6\n006FCE4404E3\nF068FD4A28D5\n3EE7CF68EFAA\n3C5FF86E8D90\nB8EB9AC4606F\n9996C0482579\n","output_type":"stream"}]},{"cell_type":"markdown","source":"If all has gone well, targets and inputs should have the same number of tokens encoded.","metadata":{}},{"cell_type":"markdown","source":"# Model building","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom keras.layers import Input, LSTM, Bidirectional, Dense\nfrom keras.models import Model\n\n# Create the input layer\nencoder_inputs = Input(shape=(None, num_input_tokens))\n\n# Create the LSTM layer:\nencoder_lstm = LSTM(256, return_sequences=True, return_state=True)\nencoder_bi = Bidirectional(encoder_lstm)\n\n# Retrieve the outputs and states:\nencoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_bi(encoder_inputs)\n\n# Per Barrow et al. \"the maximum and mean of all hidden \n#   states are concatenated with the final hidden states,\n#   and this is used as the sentence encoding.\"\n# For now... we'll just concatenate the final hidden states.\nencoder_states = [forward_h, backward_h]\n\ntest_dense = Dense(len(IBO_LABELS), activation='softmax')\nfinal_output = test_dense(encoder_outputs)\n\ntest_dense = Dense(len(IBO_LABELS), activation='softmax')\nfinal_output = test_dense(encoder_outputs)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:10:49.090367Z","iopub.execute_input":"2022-01-04T04:10:49.090621Z","iopub.status.idle":"2022-01-04T04:10:54.473898Z","shell.execute_reply.started":"2022-01-04T04:10:49.090585Z","shell.execute_reply":"2022-01-04T04:10:54.473109Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"2022-01-04 04:10:49.179810: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-04 04:10:49.180889: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-04 04:10:49.181590: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-04 04:10:49.183251: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-01-04 04:10:49.184841: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-04 04:10:49.185536: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-04 04:10:49.186168: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-04 04:10:53.630903: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-04 04:10:53.631773: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-04 04:10:53.632446: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-01-04 04:10:53.632996: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14959 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"}]},{"cell_type":"code","source":"model = Model(encoder_inputs, final_output)\nmodel.summary()\nmodel.compile(optimizer='rmsprop', \n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:10:54.475877Z","iopub.execute_input":"2022-01-04T04:10:54.476801Z","iopub.status.idle":"2022-01-04T04:10:54.501530Z","shell.execute_reply.started":"2022-01-04T04:10:54.476761Z","shell.execute_reply":"2022-01-04T04:10:54.500724Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, None, 3115)]      0         \n_________________________________________________________________\nbidirectional (Bidirectional [(None, None, 512), (None 6905856   \n_________________________________________________________________\ndense (Dense)                (None, None, 15)          7695      \n=================================================================\nTotal params: 6,913,551\nTrainable params: 6,913,551\nNon-trainable params: 0\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 50\nepochs = 50\n\nmodel.fit(one_hot_input, y, batch_size=batch_size, epochs=epochs, validation_split=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:10:54.503076Z","iopub.execute_input":"2022-01-04T04:10:54.503482Z","iopub.status.idle":"2022-01-04T04:12:20.300168Z","shell.execute_reply.started":"2022-01-04T04:10:54.503344Z","shell.execute_reply":"2022-01-04T04:12:20.299303Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"2022-01-04 04:10:55.826221: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"2022-01-04 04:10:59.568740: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8005\n","output_type":"stream"},{"name":"stdout","text":"2/2 [==============================] - 7s 1s/step - loss: 1.3809 - accuracy: 0.3648 - val_loss: 1.5142 - val_accuracy: 0.2789\nEpoch 2/50\n2/2 [==============================] - 1s 396ms/step - loss: 1.3082 - accuracy: 0.2770 - val_loss: 1.4104 - val_accuracy: 0.2789\nEpoch 3/50\n2/2 [==============================] - 1s 387ms/step - loss: 1.2720 - accuracy: 0.2736 - val_loss: 1.1251 - val_accuracy: 0.2786\nEpoch 4/50\n2/2 [==============================] - 1s 402ms/step - loss: 0.9562 - accuracy: 0.2776 - val_loss: 0.9677 - val_accuracy: 0.2789\nEpoch 5/50\n2/2 [==============================] - 1s 397ms/step - loss: 0.8453 - accuracy: 0.2714 - val_loss: 1.1207 - val_accuracy: 0.2788\nEpoch 6/50\n2/2 [==============================] - 1s 400ms/step - loss: 0.9419 - accuracy: 0.2724 - val_loss: 0.9292 - val_accuracy: 0.2789\nEpoch 7/50\n2/2 [==============================] - 1s 425ms/step - loss: 0.8612 - accuracy: 0.2726 - val_loss: 0.9804 - val_accuracy: 0.2789\nEpoch 8/50\n2/2 [==============================] - 1s 385ms/step - loss: 0.9795 - accuracy: 0.2041 - val_loss: 0.9432 - val_accuracy: 0.2789\nEpoch 9/50\n2/2 [==============================] - 1s 396ms/step - loss: 0.8660 - accuracy: 0.2791 - val_loss: 0.9185 - val_accuracy: 0.2789\nEpoch 10/50\n2/2 [==============================] - 1s 386ms/step - loss: 0.8390 - accuracy: 0.2830 - val_loss: 0.8893 - val_accuracy: 0.2789\nEpoch 11/50\n2/2 [==============================] - 1s 398ms/step - loss: 0.8140 - accuracy: 0.2813 - val_loss: 0.8950 - val_accuracy: 0.2789\nEpoch 12/50\n2/2 [==============================] - 1s 384ms/step - loss: 0.8180 - accuracy: 0.2925 - val_loss: 0.8600 - val_accuracy: 0.2789\nEpoch 13/50\n2/2 [==============================] - 1s 381ms/step - loss: 0.7967 - accuracy: 0.2785 - val_loss: 0.8795 - val_accuracy: 0.2789\nEpoch 14/50\n2/2 [==============================] - 1s 397ms/step - loss: 0.7845 - accuracy: 0.2934 - val_loss: 0.8708 - val_accuracy: 0.2791\nEpoch 15/50\n2/2 [==============================] - 1s 446ms/step - loss: 0.7852 - accuracy: 0.2836 - val_loss: 0.8986 - val_accuracy: 0.2789\nEpoch 16/50\n2/2 [==============================] - 1s 380ms/step - loss: 0.7843 - accuracy: 0.2828 - val_loss: 0.8660 - val_accuracy: 0.3066\nEpoch 17/50\n2/2 [==============================] - 1s 383ms/step - loss: 0.7659 - accuracy: 0.3175 - val_loss: 0.8689 - val_accuracy: 0.3176\nEpoch 18/50\n2/2 [==============================] - 1s 385ms/step - loss: 0.7615 - accuracy: 0.3217 - val_loss: 0.8643 - val_accuracy: 0.3192\nEpoch 19/50\n2/2 [==============================] - 1s 394ms/step - loss: 0.7534 - accuracy: 0.3169 - val_loss: 0.8778 - val_accuracy: 0.3198\nEpoch 20/50\n2/2 [==============================] - 1s 379ms/step - loss: 0.7589 - accuracy: 0.3179 - val_loss: 0.8830 - val_accuracy: 0.3204\nEpoch 21/50\n2/2 [==============================] - 1s 459ms/step - loss: 0.7573 - accuracy: 0.3230 - val_loss: 0.8531 - val_accuracy: 0.3263\nEpoch 22/50\n2/2 [==============================] - 1s 398ms/step - loss: 0.7447 - accuracy: 0.3216 - val_loss: 0.8567 - val_accuracy: 0.3281\nEpoch 23/50\n2/2 [==============================] - 1s 388ms/step - loss: 0.7395 - accuracy: 0.3222 - val_loss: 0.8783 - val_accuracy: 0.3219\nEpoch 24/50\n2/2 [==============================] - 1s 382ms/step - loss: 0.7389 - accuracy: 0.3153 - val_loss: 0.9139 - val_accuracy: 0.3187\nEpoch 25/50\n2/2 [==============================] - 1s 392ms/step - loss: 0.8780 - accuracy: 0.2294 - val_loss: 0.9703 - val_accuracy: 0.2789\nEpoch 26/50\n2/2 [==============================] - 1s 389ms/step - loss: 0.7679 - accuracy: 0.2993 - val_loss: 0.9171 - val_accuracy: 0.3070\nEpoch 27/50\n2/2 [==============================] - 1s 384ms/step - loss: 0.7620 - accuracy: 0.3167 - val_loss: 0.9053 - val_accuracy: 0.3102\nEpoch 28/50\n2/2 [==============================] - 1s 396ms/step - loss: 0.7564 - accuracy: 0.3200 - val_loss: 0.9051 - val_accuracy: 0.3110\nEpoch 29/50\n2/2 [==============================] - 1s 388ms/step - loss: 0.7540 - accuracy: 0.3214 - val_loss: 0.9027 - val_accuracy: 0.3117\nEpoch 30/50\n2/2 [==============================] - 1s 386ms/step - loss: 0.7531 - accuracy: 0.3222 - val_loss: 0.8933 - val_accuracy: 0.3133\nEpoch 31/50\n2/2 [==============================] - 1s 405ms/step - loss: 0.7483 - accuracy: 0.3226 - val_loss: 0.8927 - val_accuracy: 0.3134\nEpoch 32/50\n2/2 [==============================] - 1s 384ms/step - loss: 0.7453 - accuracy: 0.3229 - val_loss: 0.8996 - val_accuracy: 0.3131\nEpoch 33/50\n2/2 [==============================] - 1s 383ms/step - loss: 0.7457 - accuracy: 0.3234 - val_loss: 0.9069 - val_accuracy: 0.3127\nEpoch 34/50\n2/2 [==============================] - 1s 401ms/step - loss: 0.7486 - accuracy: 0.3237 - val_loss: 0.9009 - val_accuracy: 0.3127\nEpoch 35/50\n2/2 [==============================] - 1s 405ms/step - loss: 0.7439 - accuracy: 0.3237 - val_loss: 0.9008 - val_accuracy: 0.3120\nEpoch 36/50\n2/2 [==============================] - 1s 443ms/step - loss: 0.7424 - accuracy: 0.3237 - val_loss: 0.9066 - val_accuracy: 0.3102\nEpoch 37/50\n2/2 [==============================] - 1s 398ms/step - loss: 0.7458 - accuracy: 0.3234 - val_loss: 0.9005 - val_accuracy: 0.3128\nEpoch 38/50\n2/2 [==============================] - 1s 399ms/step - loss: 0.7376 - accuracy: 0.3239 - val_loss: 0.9126 - val_accuracy: 0.3102\nEpoch 39/50\n2/2 [==============================] - 1s 398ms/step - loss: 0.7385 - accuracy: 0.3236 - val_loss: 0.9022 - val_accuracy: 0.3079\nEpoch 40/50\n2/2 [==============================] - 1s 400ms/step - loss: 0.7333 - accuracy: 0.3229 - val_loss: 0.9008 - val_accuracy: 0.3125\nEpoch 41/50\n2/2 [==============================] - 1s 407ms/step - loss: 0.7333 - accuracy: 0.3255 - val_loss: 0.9002 - val_accuracy: 0.3122\nEpoch 42/50\n2/2 [==============================] - 1s 397ms/step - loss: 0.7299 - accuracy: 0.3288 - val_loss: 0.8913 - val_accuracy: 0.3156\nEpoch 43/50\n2/2 [==============================] - 1s 383ms/step - loss: 0.7308 - accuracy: 0.3269 - val_loss: 0.9062 - val_accuracy: 0.3120\nEpoch 44/50\n2/2 [==============================] - 1s 384ms/step - loss: 0.7417 - accuracy: 0.3148 - val_loss: 0.8975 - val_accuracy: 0.3135\nEpoch 45/50\n2/2 [==============================] - 1s 386ms/step - loss: 0.7088 - accuracy: 0.3306 - val_loss: 0.8904 - val_accuracy: 0.3116\nEpoch 46/50\n2/2 [==============================] - 1s 395ms/step - loss: 0.7030 - accuracy: 0.3271 - val_loss: 0.8907 - val_accuracy: 0.3170\nEpoch 47/50\n2/2 [==============================] - 1s 400ms/step - loss: 0.7337 - accuracy: 0.3272 - val_loss: 0.9177 - val_accuracy: 0.3140\nEpoch 48/50\n2/2 [==============================] - 1s 411ms/step - loss: 0.6947 - accuracy: 0.3337 - val_loss: 0.9169 - val_accuracy: 0.3081\nEpoch 49/50\n2/2 [==============================] - 1s 418ms/step - loss: 0.7159 - accuracy: 0.3223 - val_loss: 0.8932 - val_accuracy: 0.3150\nEpoch 50/50\n2/2 [==============================] - 1s 436ms/step - loss: 0.7208 - accuracy: 0.3278 - val_loss: 0.8953 - val_accuracy: 0.3138\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7efbf9f7ec10>"},"metadata":{}}]},{"cell_type":"code","source":"# Predicting segment bounds\n\n# Create the input layer\nseg_inputs = Input(shape=(None, num_input_tokens))\n\n# Create the LSTM layer:\nseg_lstm = LSTM(256, return_sequences=True, return_state=True)\nseg_bi = Bidirectional(encoder_lstm)\n\n# Retrieve the outputs and states:\nseg_outputs, forward_h, forward_c, backward_h, backward_c = sentence_encoder_bi(sentence_encoder_inputs)\n","metadata":{"execution":{"iopub.status.busy":"2022-01-04T04:12:20.301575Z","iopub.execute_input":"2022-01-04T04:12:20.301909Z","iopub.status.idle":"2022-01-04T04:12:20.339345Z","shell.execute_reply.started":"2022-01-04T04:12:20.301871Z","shell.execute_reply":"2022-01-04T04:12:20.338267Z"},"trusted":true},"execution_count":13,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_34/1763272178.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# Retrieve the outputs and states:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mseg_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_h\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackward_c\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence_encoder_bi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence_encoder_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mNameError\u001b[0m: name 'sentence_encoder_bi' is not defined"],"ename":"NameError","evalue":"name 'sentence_encoder_bi' is not defined","output_type":"error"}]},{"cell_type":"markdown","source":"","metadata":{}}]}